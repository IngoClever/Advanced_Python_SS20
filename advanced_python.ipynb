{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gstreamer Pipelines \n",
    "## von der Kamera zum DNN mit Python \n",
    "\n",
    "In diesem Jupyter Notebook findet ihr die Ausarbeitung zur Vorlesung im Wahlpflichtkurs \"Advanced Python\" an der Beuth Hochschule für Technik. Die Umsetzung ähnlicher Projekte sollhiermit erleichtert werden. \n",
    "\n",
    "Jeder Rechner mit Ubuntu 18.04 sollte die gleichen Befehle benötigen. NVIDIA AGX Xavier besitzt eine aarch64 Architektur. Der Guide funktioniert somit auch für Arm basierte Systeme. \n",
    "\n",
    "Am Ende des Notebooks versteht ihr den Umgang mit Gstreamer und könnt eine dedizierte Video-Pipeline die durch ein DNN klassifiziert wird erstellen. \n",
    "\n",
    "Im verkürzten \"Coronasemester\" SS20 habe ich leider keine weitere Energie bereit gehabt um dieses Projekt vollständiger abzuschließen. Möchte jedoch die ursprünglich geplante Bildkorrektur von einer Aufnahme im Dämmerlicht zu einer im Tageslicht in den kommenden Wochen zu Ende ausarbeiten. \n",
    "\n",
    "Die Gliederung\n",
    "1. Einrichtung der Hardware\n",
    "2. Grundlagen von Gstreamer\n",
    "3. Verbindung zu OpenCV und Einbindung des DNN\n",
    "4. dynamische Pipelines mit GstInterpipe & GstInference (von RidgeRun)\n",
    "\n",
    "<img src=\"pics/day-night.jpg\" style=\"height:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Einrichtung der Hardware\n",
    "### 1.1 - Einrichtung der SSH-Verbindung\n",
    "\n",
    "Ich nutzen den Raspberrry Pi 2 als Bridge, um preiswerte Sensoren nutzen zu können. Der Anschluss geschieht via Cross-Kabel und SSH.\n",
    "\n",
    "Rasbian und Raspberry OS sollte bereits vorkonfiguriert sein. Folgende Einstellungen sind im Konfigurationsmenü zu aktivieren.\n",
    "1. Der SSH-Server\n",
    "2. X11-forwarding (Fensterteilung über ssh)\n",
    "3. Der CSI Kameraport\n",
    "4. Um die Leistung zu verbessern, kann man den Pi in die CLI booten.\n",
    "5. Die automatische Anmeldung kann gewählt werden.\n",
    "6. Der Default-Name des Pi ist \"pi\", kann aber angepasst werden\n",
    "\n",
    "Für jedes genutzte Gerät muss eine eindeutige IP4 Adresse fest vergeben werden. Ich nutze als neues Subnetz 10.0.0.1 für den Receiver und 10.0.0.5 für den Raspberry Pi. DHCP muss abgestellt sein. Moderne Ubuntus bieten in den Netzwerkeinstellungen die Option \"Direktverbindung\". In älteren Distros nennt es sich \"Ad-Hoc\" Verbindung.\n",
    "\n",
    "Nun kann man sich mit dem Befehl \"ssh -X pi@<chosen_ip4_address>\" als Remote-Terminal, mit Fenster-Weitergabe anmelden.\n",
    "\n",
    "### 1.2 - Einrichtung des Jetson AGX Xavier mit NVIDIAS Developer Kit\n",
    "\n",
    "Hierfür benötigt man bei Ersteinrichtung ein zweites Gerät mit Ubuntu 18.04 . Die Installation setzt eine aktuelle Python Umgebung voraus. Man nutzt das kostenfreie NVIDIA Developer Kit zur Installation. Da die Software-Pakete sich noch stark verändern, sollte man von umfassender Personalisierung absehen, man wird das System oft neu aufsetzen.\n",
    "\n",
    "### 1.3 - Einrichtung der Kamera\n",
    "Um die unterstützten Auflösungen und Codierung der Kamera aufzulisten kann der Befehl\n",
    "\n",
    "_v4l2-ctl --list-formats-ext_\n",
    "\n",
    "verwendet werden. Weitere Befehle sind (X mit 0-63 ersetzen)\n",
    "\n",
    "_v4l2-compliance -d /dev/videoX -f_ \n",
    "\n",
    "_v4l2-ctl -d /dev/videoX --all_\n",
    "\n",
    "### 1.4 - Vergrößern des UDP-Buffer\n",
    "Damit mehrere Streams über den gleichen Socket versendet werden können, muss die UDP-Puffer vergrößert werden.\n",
    "Dazu kann man sich die aktuelle Größe mit den Befehlen\n",
    "\n",
    "_sysctl net.core.rmem_max_\n",
    "\n",
    "_sysctl net.core.rmem_default_\n",
    "\n",
    "ausgeben lassen. Und mit den Befehlen\n",
    "\n",
    "\n",
    "sudo sysctl -w net.core.rmem_max=8388608\n",
    "\n",
    "sudo sysctl -w net.core.wmem_max=8388608\n",
    "\n",
    "auf 8MB erweitern.\n",
    "\n",
    "### 1.5 Gstreamer installieren\n",
    "GStreamer ist in der Standardinstallation von Ubuntu enthalten, kann ansonsten aber über das folgende Paket installiert werden:\n",
    "\n",
    "_sudo apt-get install libgstreamer1.0-0_\n",
    "\n",
    "\n",
    "Empfehlenswert ist weiterhin noch das Paket:\n",
    "\n",
    "\n",
    "_sudo apt-get install gstreamer1.0-tools_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Grundlagen von Gstreamer\n",
    "\n",
    "Gstreamer (https://gstreamer.freedesktop.org/) ist Open Source. In Gstreamer bildet man komplexe Pipelines zur  Erstellung von \"Streaming Media Apllication\".\n",
    "\n",
    "Eine Gstreamer Pipeline ist modular aufgebaut. Sie wird aus den vielen unterschiedlichen PlugIns zusammen gehängt.\n",
    "Gstreamer kann in vielen verschiedenen Programmiersprachen bearbeitet werden. \n",
    "https://gstreamer.freedesktop.org/bindings/\n",
    "\n",
    "(Der Standardbaustein eines Muxer oder Demuxer ist im unteren Bild nicht abgebildet.)\n",
    "\n",
    "<img src=\"pics/thread-buffering.png\" style=\"width:1000px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Kamerastream vom RaspberryPi senden\n",
    "- Auch die CSI Kamera im Raspberry wird über die Video4Linux2 Treiber angesprochen. \n",
    "- Die Kameras liegen dabei im Verzeichnes /dev mit der Bezeichnung video0 bis video63\n",
    "- Videoformat sowie Chroma (Interlaced YUV 4:2:0) sind Mindestangaben!\n",
    "- um den Videostrem im \"Real Time Streaming Protocol\" zu versenden wird die _rtpvrawpay_ Node genutzt.\n",
    "- die udpsink wandelt den Payload in UPD-konforme Pakete um.\n",
    "- Wir möchten keinen Sync, das heißt Pakete die nicht verarbeitet werden können werden verworfen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "gst-launch-1.0 -ev v4l2src \\\n",
    "device=/dev/video0 !      \\\n",
    "'video/x-raw, width=128, height=96,' \\\n",
    "'format=I420, framerate=10/1'!       \\\n",
    "rtpvrawpay !              \\\n",
    "udpsink host=10.0.0.1 port=5000      \\\n",
    "sync=false async=false\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Kamerastream empfangen\n",
    "Auf der Empfängerseite muss der Stream in der umgekehrten Reihenfolge mit den entsprechenden Modulen umgewandelt werden.\n",
    "Als End-Node wird \"xvimagesink\" genutzt. die ein X-Fenster öffnet um das Video abzuspielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "gst-launch-1.0 udpsrc port=5000 !\n",
    "'application/x-rtp, media=(string)video,'\n",
    "'encoding-name=(string)RAW, sampling=YCbCr-4:2:0,'\n",
    "'width=(string)128, height=(string)96' !\n",
    "rtpvrawdepay ! xvimagesink\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Informationen der Shell, bei setzen des -ev flag können ebenfalls nötig sein. Zum Beispiel braucht man den timestamp-offset bei der Synchronisation und dem scrollen durch Streams. Die Informationen jedes Moduls werden hierbei aufgeführt.\n",
    "<img src=\"pics/pi_gstreamer.png\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Gstreamer zu OpenCV\n",
    "### 3.1 - Gst-Python\n",
    "Für das aufsetzen des Videostream bietet sich Python an. Hierbei kann man einen Factory Pattern nutzen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gi\n",
    "from gi.repository import Gst, GObjekt\n",
    "GObject.threads_init\n",
    "Gst.init(None)\n",
    "\n",
    "class SimplePipeline(object):\n",
    "    def __init__(self):\n",
    "        self.pipeline = Gst.Pipeline()\n",
    "        self.filesrc = Gst.ElementFactory.make('filesrc')\n",
    "        self.oggdemux = Gst.ElementFactory.make('oggdemux')\n",
    "\n",
    "        self.pipeline.add(self.filesrc)         # Module dem Bauplan zufügen\n",
    "        self.pipeline.add(self.oggdemux)\n",
    "        self.pipeline.add(self.theoradec)\n",
    "        self.pipeline.add(self.videoconvert)\n",
    "        self.pipeline.add(self.xvimagesink)\n",
    "\n",
    "        self.filesrc.link(self.oggdemux)        # Module verlinken\n",
    "        self.oggdemux.link(self.theoracodec)\n",
    "        self.theoracodec.link(self.videoconvert)\n",
    "        self.videoconvert.link(self.xvimagesink)\n",
    "\n",
    "loop = GObject.MainLoop()\n",
    "\n",
    "p = SimplePipeline()\n",
    "\n",
    "p.filesrc.set_property('location', video 'video.ogv')\n",
    "p.pipeline.set_state(Gst.State.PLAYING)\n",
    "loop.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oder, je nachdem, was man übersichtlicher findet auch string-parsing. Das Debugging größerer Pipelines kann sonst umständlich werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePipeline(object)\n",
    "    def __init__(self):\n",
    "        pipe_desc = ('filesrc name=src !'\n",
    "                    'oggdemux ! theoradec !'\n",
    "                    'videoconvert ! xvimagesink')\n",
    "        self.pipeline = Gst.parse_launch(pipe_desc)\n",
    "        self-filesrc = self.pipeline.get_by_name('src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - OpenCV um Videostreams zu nutzen\n",
    "OpenCV ist zurecht der Standard für Computer Vision. Es beinhaltet über 2500 Module mit absolut aktuellem Bezug.\n",
    "https://docs.opencv.org/master/d9/df8/tutorial_root.html\n",
    "Wir nutzen es um die Gstreamer Pipeline zu bearbeiten. Den Videostream anzupassen und durch ein DNN zu bewerten. Die Ergebnisse werden ebenfalls mithilfe von OpenCV herausgestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der benötigten Pakete\n",
    "from __future__ import print_function\n",
    "from imutils.video import WebcamVideoStream # https://github.com/jrosebr1/imutils/ für Threads\n",
    "import cv2                                  # https://github.com/opencv/opencv\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Als erstes laden wir die trainierten Gewichte, und die Erkennungsklassen in OpenCV.\n",
    "Wir sehen hier im Modellnamen, dass Bilder zur Verarbeitung die Maße 300px x 300 px haben müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# die Erkkennungslabel müssen angelegt werden. (z.B. 1: 'random' etc.)\n",
    "classNames = { 0: 'face' }            \n",
    "    \n",
    "# Wir laden das Caffe oder Tensorflow Modell\n",
    "net = cv2.dnn.readNetFromCaffe('resnet10/deploy.prototxt', 'resnet10/res10_300x300_ssd_iter_140000.caffemodel')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Gstreamer Pipeline definieren\n",
    "\n",
    "Die Initialisierung der Pipeline benötigt, wie oben besprochen weitere Module. Um den Videostream einzulesen werden zwei weitere Konvertierungen, in das Format _NV12_ und von dort nach _RGB_ vorgenommen. Weitere Informationen hierzu:\n",
    "https://gstreamer.freedesktop.org/documentation/additional/design/mediatype-video-raw.html?gi-language=c#page-description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive():\n",
    "    gst_str = ('udpsrc port=5000 caps = \"application/x-rtp, media=(string)video, clock-rate=(int)90000, '\n",
    "               'encoding-name=(string)RAW, sampling=YCbCr-4:2:0,depth=(string)8,colorimetry=(string)BT601-5, '\n",
    "               'width=(string)720, height=(string)576, payload=(int)96, a-framerate=10\" ! '\n",
    "               'rtpvrawdepay ! videoconvert ! '\n",
    "               'video/x-raw, format=(string)NV12! '\n",
    "               'videoconvert ! appsink emit-signals=True')\n",
    "    return  WebcamVideoStream(gst_str, cv2.CAP_GSTREAMER).start()# mit Threading\n",
    "    #return cv2.VideoCapture(gst_str, cv2.CAP_GSTREAMER)  # ohne Threading\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir passen das zu öffnende Fenster ein wenig an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_NAME = 'CameraDemo'\n",
    "    \n",
    "def open_window():\n",
    "    cv2.startWindowThread()\n",
    "    cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(WINDOW_NAME, 720, 576)\n",
    "    cv2.moveWindow(WINDOW_NAME, 0, 0)\n",
    "    cv2.setWindowTitle(WINDOW_NAME, 'RTSP-Camera Demo for Xavier AGX')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Das Video mit dem DNN zu untersuchen muss es in eine (Bild)Datei mit 300px x 300px umgewandelt werden\n",
    "https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/\n",
    "Ergebnisse mit mehr als 20% Korrelation werden durch ein Rechteck markiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_cam(cap):\n",
    "    while True:\n",
    "       \n",
    "        img = cap.read() # Folgebild von Kamera lesen\n",
    "        frame = img\n",
    "\n",
    "        # Abmessungen auslesen und in einen \"Blob\" umwandeln\n",
    "        (h, w) = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
    "            (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "        # Den Blob durchs DNN analysieren lassen und die Vorhersagen in detections \n",
    "        # abspeichern\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        \n",
    "        for i in range(0, detections.shape[2]):\n",
    "            # Die Wahrscheinlichkeit der Vorhersagen iterativ abfragen.\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            # schwache Vorhersagen (mit einem Wert unter 20%) filtern\n",
    "            if confidence < 0.2:\n",
    "                continue\n",
    "            # (x, y)- Koordinaten zur Berechnung eines Rechtecks nutzen\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # Umgranzungskästen zeichen und die Wahrscheinlichkeit als Text \n",
    "            # oben drüber schreiben\n",
    "            text = \"{:.2f}%\".format(confidence * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                (0, 0, 255), 2)\n",
    "            cv2.putText(frame, text, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "        # Fenster öffnen \n",
    "        cv2.imshow(WINDOW_NAME, frame)\n",
    "\n",
    "def main():\n",
    "    # Verbindung öffnen\n",
    "    cap = receive()\n",
    "    # Fenster öffnen\n",
    "    open_window()\n",
    "    # Kamerastream einlesen und analysieren\n",
    "    read_cam(cap)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Funktion `recieve()` wurde eine OpenCV Funktion ersetzt. Der zugehörige Code steht im folgenden Feld. Hiermit wird ein eigener Thread für den Videostream geöffnet. Bei der Arbeit mit mehreren Videostreams ist diese Ersetzung essentiell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for threaded videostreams by Adrian Rosebrock\n",
    "from threading import Thread\n",
    "import cv2\n",
    "class WebcamVideoStream:\n",
    "    def __init__(self, src=0):\n",
    "        # initialize the video camera stream and read the first frame\n",
    "        # from the stream\n",
    "        self.stream = cv2.VideoCapture(src)\n",
    "        (self.grabbed, self.frame) = self.stream.read()\n",
    "        # initialize the variable used to indicate if the thread should\n",
    "        # be stopped\n",
    "        self.stopped = False\n",
    "        \n",
    "    def start(self):\n",
    "        # start the thread to read frames from the video stream\n",
    "        Thread(target=self.update, args=()).start()\n",
    "        return self\n",
    "    def update(self):\n",
    "    #keep looping infinitely until the thread is stopped\n",
    "        while True:\n",
    "            # if the thread indicator variable is set, stop the thread\n",
    "            if self.stopped:\n",
    "                return\n",
    "            # otherwise, read the next frame from the stream\n",
    "            (self.grabbed, self.frame) = self.stream.read()\n",
    "    def read(self):\n",
    "        # return the frame most recently read\n",
    "        return self.frame\n",
    "    def stop(self):\n",
    "        # indicate that the thread should be stopped\n",
    "        self.stopped = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergenbis zeigt die Gesichtserkennung im Fenster. Der genutzte Facenet-Graph ist zwar schnell, aber noch nicht besonders zuverlässig.\n",
    "<img src=\"pics/Face.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Nutzung von GstInference & weiteren Modulen von RidgeRun\n",
    "RidgeRun bietet für Gstreamer weitere fortgeschrittene PlugIns an, mit denen sich efiizient dynamische Pipelines mit gleicher Funktion wie dem oberen Beispiel erstllen lassen.\n",
    "https://developer.ridgerun.com/wiki/index.php?title=Main_Page\n",
    "Diese Methode hat den Vorteil dass keine Kopie auf der Festplatte benötigt wird.\n",
    "\n",
    "Für diesen Vortrag nutzen wir Gst-Inference\n",
    "https://developer.ridgerun.com/wiki/index.php?title=GstInference/Getting_started/Building_the_plugin\n",
    "\n",
    "Dieses Tol benötigt zur Implementierung der verschiedenen DNN-Protokolle und Architekturen r²Inference.\n",
    "\n",
    "https://developer.ridgerun.com/wiki/index.php?title=R2Inference/Getting_started/Building_the_library\n",
    "\n",
    "### 4.1 - Bewertung eines Kamerastream durch DNN mithilfe von GstInference\n",
    "Damit das untere Bash-Skript funktioniert muss eine USB-Kamera angschlossen sein, die 1280px x 720px Wiedergabe  unterstützt. Um den Einstieg (in RidgeRun Module) zu erleichtern wird eine GUI angeboten\n",
    "https://developer.ridgerun.com/wiki/index.php?title=GstInference/Example_pipelines_with_hierarchical_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# gstreamer TinyYolo GstInference\n",
    "CAMERA='/dev/video0'\n",
    "MODEL_LOCATION='TinyYoloV2_TensorFlow/graph_tinyyolov2_tensorflow.pb'\n",
    "INPUT_LAYER='input/Placeholder'\n",
    "OUTPUT_LAYER='add_8'\n",
    "LABELS='TinyYoloV2_TensorFlow/labels.txt'\n",
    "gst-launch-1.0 v4l2src device=$CAMERA ! \"video/x-raw, width=1280, height=720\" ! tee name=t \\\n",
    "t. ! videoconvert ! videoscale ! queue ! net.sink_model \\\n",
    "t. ! queue ! net.sink_bypass \\\n",
    "tinyyolov2 name=net model-location=$MODEL_LOCATION backend=tensorflow backend::input-layer=$INPUT_LAYER backend::output-layer=$OUTPUT_LAYER \\\n",
    "net.src_bypass ! videoconvert ! detectionoverlay labels=\"$(cat $LABELS)\" font-scale=1 thickness=2 ! videoconvert ! xvimagesink sync=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Bashskript öffnet ein Fenster. Zumindest das genutzte Framework ist dabei um einiges schneller als die OpenCV Lösung. Es verarbeitet die doppelte Auflösung. (Genaue Aussagen über die Effektivität kann ich jedoch nicht treffen.)\n",
    "<img src=\"pics/Yolo.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weitere erwähnenswerte Frameworks mit fortgeschrittenen Funktionen sind **Deepstream** von Nvidia.\n",
    "https://developer.nvidia.com/deepstream-sdk\n",
    "\n",
    "Sowie der NNStreamer, ehemals von Samsung, der sogar Tizen OS unterstützt.\n",
    "https://github.com/nnstreamer/nnstreamer\n",
    "\n",
    "### 4.2 - GstInterpipe und Gstreamer-Daemon zur Erstellung komplexer Pipelines\n",
    "<img src=\"pics/Gstinterpipe-simple-diag.png\" style=\"width:1000px;\">\n",
    "\n",
    "GstInterpipe erlaubt die Kommunikation zweiwe Pipelines. Es beinhaltet die _interpipesink_ und die _interpipesrc_.\n",
    "\n",
    "Damit wird es möglich in einer High-Level Anwendung auf die Eigenschaften verschiedener Pipelines zentral, Einfluß zu nehmen. Es garantiert die Kompatibilität der Caps in den genutzten Streams. \n",
    "\n",
    "Ebenso vereinfacht der Gstreamer-Daemon den Umgang mit mehreren Pipelines. Durch ihn wird es ausserdem einfacher Events und Pufferanweisungen zu synchronisieren.\n",
    "\n",
    "Sowie Gst-Interpipe\n",
    "https://developer.ridgerun.com/wiki/index.php?title=GstInterpipe_-_Building_and_Installation_Guide\n",
    "\n",
    "und den Gstreamer-Daemon\n",
    "https://developer.ridgerun.com/wiki/index.php?title=GStreamer_Daemon_-_Building_GStreamer_Daemon\n",
    "\n",
    "Das Beispiel verdeutlicht die Funktionsweise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "gst-client\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline_create testpipe videotestsrc name=vts ! autovideosink\n",
    "\n",
    "# Play the pipeline\n",
    "pipeline_play testpipe\n",
    "\n",
    "# Change a property\n",
    "element_set testpipe vts pattern ball\n",
    "\n",
    "# Stop the pipeline\n",
    "pipeline_stop testpipe\n",
    "\n",
    "# Destroy the pipeline \n",
    "pipeline_delete testpipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Bringe alles zusammen\n",
    "Das folgende Bashskript funktionert leider noch nicht. Wenn funktional werden zwei Kameras geöffnet, eine IP-Cam und eine USB-Cam. Beide Videostreams werden durch ein DNN bewertet und markiert. Beide Videostreams werden auf dem Gerät simultan ausgegeben. Dazu werden alle vorher benannten RidgeRun Module genutzt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/env bash\n",
    "echo -e \"\\n ====== Jetson Xavier Multi-Camera AI Demo ====== \\n\"\n",
    "# Configure Demo Variables\n",
    "ROOM_ID=mvdemo\n",
    "DOWNSCALE_CAPS='video/x-raw,width=480,height=480,format=I420,pixel-aspect-ratio=1/1'\n",
    "DOWNSCALE_NVMM_CAPS='video/x-raw(memory:NVMM),width=480,height=480,format=I420,pixel-aspect-ratio=1/1'\n",
    "TY_MODEL_LOCATION='TinyYoloV2_TensorFlow/graph_tinyyolov2_tensorflow.pb'\n",
    "TY_INPUT_LAYER='input/Placeholder'\n",
    "TY_OUTPUT_LAYER='add_8'\n",
    "TY_LABELS='TinyYoloV2_TensorFlow/labels.txt'\n",
    "TY_OUT_CAPS='video/x-raw,width=1280,height=720,format=RGBA'\n",
    "INC_MODEL_LOCATION='/InceptionV1_TensorFlow/graph_inceptionv1_tensorflow.pb'\n",
    "INC_INPUT_LAYER='input'\n",
    "INC_OUTPUT_LAYER='InceptionV1/Logits/Predictions/Reshape_1'\n",
    "INC_LABELS='InceptionV1_TensorFlow/imagenet_labels.txt'\n",
    "INC_OUT_CAPS='video/x-raw,width=720,height=576,format=BGRx'\n",
    "END_MSJ=\"!!! Jetson Xavier Multi-Camera AI Demo Finished !!!\"\n",
    "\n",
    "# Camera_1 Pipeline: InceptionV1 Classification Inference applied with GstInference plugin using GPU accelerated TensorFlow.\n",
    "\n",
    "CAM1=\"udpsrc port=5000 ! 'application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)RAW, sampling=(string)YCbCr-4:2:0, depth=(string)8, colorimetry=(string)BT601-5, width=(string)720, height=(string)576, payload=(int)96, a-framerate=(string)10' ! rtpvrawdepay ! \\\n",
    "tee name=tee_1 tee_1. ! queue max-size-buffers=1 leaky=downstream ! nvvidconv ! video/x-raw ! inc_1.sink_bypass tee_1. ! \\\n",
    "queue max-size-buffers=1 leaky=downstream ! nvvidconv ! inc_1.sink_model inceptionv1 name=inc_1 backend=tensorflow \\\n",
    "model-location=$INC_MODEL_LOCATION backend::input-layer=$INC_INPUT_LAYER backend::output-layer=$INC_OUTPUT_LAYER inc_1.src_bypass ! \\\n",
    "queue max-size-buffers=1 leaky=downstream ! $INC_OUT_CAPS ! classificationoverlay labels=\"$(cat $INC_LABELS)\" font-scale=4 thickness=4 ! \\\n",
    "queue max-size-buffers=1 leaky=downstream ! nvvidconv ! $DOWNSCALE_NVMM_CAPS ! nvvidconv ! $DOWNSCALE_CAPS ! \\\n",
    "queue max-size-buffers=1 leaky=downstream ! interpipesink name=psink_cam1 caps=$DOWNSCALE_CAPS sync=false qos=false async=false \\\n",
    "enable-last-sample=false drop=true max-buffers=2 forward-eos=false forward-events=false\\\n",
    "\"\n",
    "\n",
    "# Camera_2 Pipeline: TinyYoloV2 Detection Inference applied with GstInference plugin using GPU accelerated TensorFlow.\n",
    "CAM2=\"v4l2src device=/dev/video0 ! video/x-raw, width=1280, height=720 ! \\\n",
    "tee name=tee_2 tee_2. ! queue max-size-buffers=1 leaky=downstream ! nvvidconv ! video/x-raw ! ty_2.sink_bypass tee_2. ! \\\n",
    "queue max-size-buffers=1 leaky=downstream ! nvvidconv ! ty_2.sink_model tinyyolov2 name=ty_2 backend=tensorflow \\\n",
    "model-location=$TY_MODEL_LOCATION backend::input-layer=$TY_INPUT_LAYER backend::output-layer=$TY_OUTPUT_LAYER ty_2.src_bypass ! \\\n",
    "queue max-size-buffers=1 leaky=downstream ! detectionoverlay labels=\"$(cat $TY_LABELS)\" font-scale=4 thickness=4 ! $TY_OUT_CAPS ! \\\n",
    "queue max-size-buffers=1 leaky=downstream ! nvvidconv ! $DOWNSCALE_NVMM_CAPS ! nvvidconv ! $DOWNSCALE_CAPS ! \\\n",
    "queue max-size-buffers=1 leaky=downstream ! interpipesink name=psink_cam2 caps=$DOWNSCALE_CAPS sync=false qos=false async=false \\\n",
    "enable-last-sample=false drop=true max-buffers=2 forward-eos=false forward-events=false\\\n",
    "\"\n",
    "\n",
    "# Display Grid Pipeline\n",
    "GRID=\"interpipesrc name=grid_cam1 listen-to=psink_cam1 format=time max-bytes=0 block=false max-latency=0 is-live=true allow-renegotiation=false \\\n",
    "enable-sync=true accept-events=false accept-eos-event=false ! queue ! \\\n",
    "perf name=perf1 ! textoverlay name=textoverlay1 text=Classification-Inference-camera_1 shaded-background=true ! mixer.sink_0 \\\n",
    "interpipesrc name=grid_cam2 listen-to=psink_cam2 format=time max-bytes=0 block=false max-latency=0 is-live=true allow-renegotiation=false \\\n",
    "enable-sync=true accept-events=false accept-eos-event=false ! queue ! \\\n",
    "perf name=perf2 ! textoverlay name=textoverlay2 text=Detection-Inference-camera_2 shaded-background=true ! mixer.sink_1 \\\n",
    "videomixer \\\n",
    "name=mixer sink_0::xpos=0 sink_0::ypos=0 sink_1::xpos=577 sink_1::ypos=0  \\\n",
    "queue max-size-buffers=1 leaky=downstream ! videoconvert ! queue max-size-buffers=1 leaky=downstream ! \\\n",
    "nvvidconv ! nvoverlaysink enable-last-sample=false sync=false async=false\\\n",
    "\"\n",
    "\n",
    "# Multi-Stream Pipeline\n",
    "GST_MULTI_STREAM=\"interpipesrc name=multivid listen-to=psink_cam1 format=time max-bytes=0 block=false max-latency=0 is-live=true allow-renegotiation=true \\\n",
    "enable-sync=true accept-events=false accept-eos-event=false ! videoscale ! videoconvert ! autovideosink\\\n",
    "\"\n",
    "\n",
    "echo -e \"\\n ====> Creating Pipelines \\n\"\n",
    "gstd-client pipeline_create camera1 $CAM1 &> log\n",
    "gstd-client pipeline_create camera2 $CAM2 &> log\n",
    "gstd-client pipeline_create grid $GRID &> log\n",
    "gstd-client pipeline_create MultiVid $GST_MULTI_STREAM &> log\n",
    "\n",
    "# Change pipelines to PLAYING STATE\n",
    "echo -e \"\\n ====> Starting Pipelines \\n\"\n",
    "gstd-client pipeline_play camera1 &> log\n",
    "gstd-client pipeline_play camera2 &> log\n",
    "gstd-client pipeline_play grid &> log\n",
    "gstd-client pipeline_play MultiVid &> log\n",
    "\n",
    "echo -e \"\\n ====> Multi-Videostream playing! \\n\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Am Ende dieses Projekt steht das Verständnis zur Nutzung mehrerer Kameras im Direktanschluss und als IP-Kamera. Und die Auswertung der Videodaten durch ein DNN auf einem mobilen Gerät. Ich hoffe, dass dieser Ansatzpunkt verständlich und umfassend genug herübergebracht wurde. Für weitere Fragen und natürlich Anregungen bin ich weiterhin erreichbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "rSupZ",
   "launcher_item_id": "cvGhe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
